{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "L8kbFRoYj3BI",
        "Z_MTwn-YQ37H",
        "_XcEZWRU7j8b",
        "qBn6tAjx39E7"
      ],
      "authorship_tag": "ABX9TyP8rMXm+f9MM2IogQXl3HI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giordanovitale/Prado-Museum-CNN/blob/main/Prado_Artists.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   [Data Augmentation Techniques](https://medium.com/ymedialabs-innovation/data-augmentation-techniques-in-cnn-using-tensorflow-371ae43d5be9#8be0)\n",
        "2.   [Model Architectures](https://medium.com/@navarai/unveiling-the-diversity-a-comprehensive-guide-to-types-of-cnn-architectures-9d70da0b4521)\n",
        "3. [EfficientNet](https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142)"
      ],
      "metadata": {
        "id": "WbjcyTneoQAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 - Load the necessary libraries"
      ],
      "metadata": {
        "id": "o8R_We7Bjw9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Source: https://www.kaggle.com/datasets/maparla/prado-museum-pictures"
      ],
      "metadata": {
        "id": "wzU4x8i_tk_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7FrmpviCjilL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import polars as pl\n",
        "\n",
        "import tqdm\n",
        "import os\n",
        "import requests\n",
        "\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "# import visualkeras as vk\n",
        "\n",
        "from scipy.optimize import fsolve\n",
        "from math import exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.layers import Layer\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, \\\n",
        "    AveragePooling2D, BatchNormalization, ReLU, PReLU, ZeroPadding2D, \\\n",
        "    GlobalAveragePooling2D, Input, DepthwiseConv2D, Add, Activation, Lambda, RandomFlip, \\\n",
        "    Rescaling, RandomContrast, RandomZoom\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_v2_preproccessing\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as efficientnet_preproccessing\n",
        "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B3\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg_preproccessing\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocessing\n",
        "\n",
        "# Get rid of the extra stuff that gets downloaded\n",
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (224,224,3)"
      ],
      "metadata": {
        "id": "O4u7vtjkI7bk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Helper Functions"
      ],
      "metadata": {
        "id": "L8kbFRoYj3BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_url(args):\n",
        "    \"\"\"\n",
        "    Downloads a file from an url\n",
        "    :param args: Tuple containing the url and filename\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    url, filename = args[0], args[1]\n",
        "    try:\n",
        "      r = requests.get(url)\n",
        "      if r.status_code != 404:\n",
        "        with open(filename, \"wb\") as f:\n",
        "          f.write(r.content)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"Exception in download_url():\", e)"
      ],
      "metadata": {
        "id": "OYyBBIO-jvgM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_parallel(args):\n",
        "    \"\"\"\n",
        "    Downloads urls in parallel\n",
        "    :param args: List of tuples containing the url and filename\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    cpus = cpu_count()\n",
        "    threadpool = ThreadPool(cpus)\n",
        "    with tqdm.tqdm(total=len(args), desc=\"Downloading images\") as pbar:\n",
        "        for _ in threadpool.imap_unordered(download_url, args):\n",
        "            pbar.update(1)\n",
        "    threadpool.close()\n",
        "    threadpool.join()"
      ],
      "metadata": {
        "id": "6lOeid62TQSE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Load the dataset using Kaggle API"
      ],
      "metadata": {
        "id": "MaMrNaJvoe2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Username and Key have been secreted. Replace `userdata.get('KAGGLE_USERNAME')` and `userdata.get('KAGGLE_KEY')`with your username and key, respectively."
      ],
      "metadata": {
        "id": "wZrGs5uD2s0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KAGGLE_USERNAME\"] = \"userdata.get('KAGGLE_USERNAME')\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"userdata.get('KAGGLE_KEY')\"\n",
        "!kaggle datasets download maparla/prado-museum-pictures -f prado.csv\n",
        "!unzip prado.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBj68-vEvZLa",
        "outputId": "9f6b43c6-4fc3-4a28-84af-c3622bc5e709"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/maparla/prado-museum-pictures\n",
            "License(s): MIT\n",
            "Downloading prado.csv.zip to /content/drive/MyDrive/AMD\n",
            " 76% 14.0M/18.3M [00:00<00:00, 63.9MB/s]\n",
            "100% 18.3M/18.3M [00:00<00:00, 66.6MB/s]\n",
            "Archive:  prado.csv.zip\n",
            "  inflating: prado.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataframe from the unzipepd csv file."
      ],
      "metadata": {
        "id": "RZ4NWN4a26ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(\"prado.csv\"))"
      ],
      "metadata": {
        "id": "FRAesYqqwHCr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since no target class has been defined from the project assignment, I have to decide it. After a careful inspection of the columns, I found out that the more suitable ones are `author` and `technical_sheet_tecnica`. The latter seems more intriguing as it has more observations, thus being more suitable to big data algorithms."
      ],
      "metadata": {
        "id": "d60cuROa2-o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['author'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKYs7m-XwOyB",
        "outputId": "f441ad4e-9d91-4c6f-f5dd-94acd095c2d1"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author\n",
              "Anónimo                                                                       2698\n",
              "Goya y Lucientes, Francisco de                                                1080\n",
              "Bayeu y Subías, Francisco                                                      446\n",
              "Haes, Carlos de                                                                326\n",
              "Pizarro y Librado, Cecilio                                                     290\n",
              "                                                                              ... \n",
              "Malombra, Pietro                                                                 1\n",
              "Taller de Bellini, Giovanni                                                      1\n",
              "Mattioli, Ludovico -Dibujante- (Autor de la obra original: Cignani, Carlo)       1\n",
              "Ricci, Marco                                                                     1\n",
              "García, Sergio                                                                   1\n",
              "Name: count, Length: 2560, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['technical_sheet_tecnica'].value_counts().sort_values(ascending=False)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD4-MW-bwPga",
        "outputId": "15f34df9-309d-4242-f7ef-e76c6c0bda4b"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception in download_url(): [Errno 2] No such file or directory: 'data/pencil/59825391-c4b9-4f84-8d6f-413fa045a3ea.jpg'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "technical_sheet_tecnica\n",
              "Óleo                    4156\n",
              "Acuñación               1118\n",
              "Esculpido                550\n",
              "Lápiz compuesto          476\n",
              "Clarión; Lápiz negro     396\n",
              "Albúmina                 395\n",
              "Sanguina                 372\n",
              "Lápiz                    259\n",
              "Lápiz negro              237\n",
              "Pluma; Tinta parda       214\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce the data set by keeping observations belonging to the 4 classes of interest only."
      ],
      "metadata": {
        "id": "RaNVQhcOcjAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['technical_sheet_tecnica'].isin(['Óleo',\n",
        "                                            'Acuñación',\n",
        "                                            'Esculpido',\n",
        "                                            'Lápiz compuesto'])]"
      ],
      "metadata": {
        "id": "JjRARJodH5or"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B467V520Iwcn",
        "outputId": "ade3207b-4d2c-4c2d-86fd-c08dc99ef0b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6300, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to obtain the JPGs images, we need to start from the given URL column `work_image_url`."
      ],
      "metadata": {
        "id": "0iHqoZHs4btw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['work_id'] = df['work_image_url'].apply(lambda x: x.split('/')[-1])"
      ],
      "metadata": {
        "id": "bvORo-571oOX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the folders into which the images will be stored, according to their respective class."
      ],
      "metadata": {
        "id": "GO_mspTpBiTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The important columns are:\n",
        "`['work_image_url', 'work_id', 'technical_sheet_tecnica']`"
      ],
      "metadata": {
        "id": "FC_ZTuARczzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "NvXIGlFrRsRR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "esp_techniques = ['Óleo', 'Acuñación', 'Esculpido', 'Lápiz compuesto']\n",
        "eng_techniques = ['oil', 'minting', 'sculpture', 'pencil']"
      ],
      "metadata": {
        "id": "0MzHBZFvTgpP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rm -rf data"
      ],
      "metadata": {
        "id": "95J0G6lWUtRU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for technique in eng_techniques:\n",
        "  !mkdir -p data/$technique"
      ],
      "metadata": {
        "id": "dUdTNYO1RzSa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model_logs"
      ],
      "metadata": {
        "id": "1i5NdKk6dNVq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJAjz_Hfczwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BF5ZPrAEeEHL"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05d-eS5ynl5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_list = list()\n",
        "\n",
        "for esp_technique, eng_technique in zip(esp_techniques, eng_techniques):\n",
        "\n",
        "  image_urls = list(df['work_image_url'].loc[df['technical_sheet_tecnica'] == esp_technique])\n",
        "  image_names = list(df['work_id'].loc[df['technical_sheet_tecnica'] == esp_technique])\n",
        "  image_names = [f\"data/{eng_technique}/\" + fn for fn in image_names]\n",
        "\n",
        "  for url, fn in zip(image_urls, image_names):\n",
        "    download_list.append((url, fn))"
      ],
      "metadata": {
        "id": "LgleWlJtoExf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mI9BfgmV9S9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(download_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbWz_pknorCZ",
        "outputId": "e762ca34-9391-46a0-fa12-d6aba06b3008"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6300"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_parallel(download_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFc2YbyFosxg",
        "outputId": "2482bb73-b580-431d-c268-9472a00254e8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading images: 100%|██████████| 6300/6300 [2:03:55<00:00,  1.18s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Imbalance"
      ],
      "metadata": {
        "id": "Z_MTwn-YQ37H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights"
      ],
      "metadata": {
        "id": "McYkNoNiMZ40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_oils = df.loc[df['technical_sheet_tecnica']=='Óleo'].shape[0]\n",
        "num_mintings = df.loc[df['technical_sheet_tecnica']=='Acuñación'].shape[0]\n",
        "num_sculptures = df.loc[df['technical_sheet_tecnica']=='Esculpido'].shape[0]\n",
        "num_pencils = df.loc[df['technical_sheet_tecnica']=='Lápiz compuesto'].shape[0]\n",
        "\n",
        "num_total = df.shape[0]"
      ],
      "metadata": {
        "id": "Ds6y2fbTMz9w"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = {\n",
        "    0: num_oils/num_total,\n",
        "    1: num_mintings/num_total,\n",
        "    2: num_sculptures/num_total,\n",
        "    3: num_pencils/num_total\n",
        "}\n",
        "\n",
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDE6xgiyMCSn",
        "outputId": "0688b58b-a2a5-4516-80ad-78c5dfb3bbf6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.6596825396825396,\n",
              " 1: 0.17746031746031746,\n",
              " 2: 0.0873015873015873,\n",
              " 3: 0.07555555555555556}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Temporarily use loaded data and mount"
      ],
      "metadata": {
        "id": "_XcEZWRU7j8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/AMD\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drTVncya7sQf",
        "outputId": "eec0d6cc-7b5c-4761-b10a-594c921f2c60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "data  Prado-Artists.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = tf.keras.utils.image_dataset_from_directory('/content/drive/MyDrive/AMD/data',\n",
        "                                                                label_mode='int',\n",
        "                                                                color_mode='rgb',\n",
        "                                                                batch_size=32,\n",
        "                                                                image_size=(224, 224),\n",
        "                                                                shuffle=True,\n",
        "                                                                seed=42,\n",
        "                                                                validation_split=0.2,\n",
        "                                                                subset='both',\n",
        "                                                                labels='inferred',\n",
        "                                                                class_names=['oil', 'minting', 'sculpture', 'pencil'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUmID9Kk80Yv",
        "outputId": "363b023c-9fe4-4a13-c617-b860c7ada900"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5935 files belonging to 4 classes.\n",
            "Using 4748 files for training.\n",
            "Using 1187 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Model 1: ZFNet"
      ],
      "metadata": {
        "id": "jyowUsuDJFrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/images/data_augmentation"
      ],
      "metadata": {
        "id": "dhN1PODLEN6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the link above:\n",
        "\n",
        "> Note: Data augmentation is inactive at test time so input images will only be augmented during calls to Model.fit (not Model.evaluate or Model.predict).\n",
        "\n"
      ],
      "metadata": {
        "id": "eoZ2PpzPGKbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4"
      ],
      "metadata": {
        "id": "-lewV5opGf1U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet = Sequential([\n",
        "\n",
        "    # Preprocessing layers\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.RandomFlip(mode='horizontal_and_vertical',\n",
        "                               input_shape=INPUT_SHAPE,\n",
        "                               name='Random_Horizontal_And_Vertical_Flip'),\n",
        "    tf.keras.layers.RandomRotation(factor=0.2,\n",
        "                                   input_shape=INPUT_SHAPE,\n",
        "                                   name='Random_Rotation'),\n",
        "    tf.keras.layers.RandomContrast(factor=0.2,\n",
        "                                   input_shape=INPUT_SHAPE,\n",
        "                                   name='Random_Contrast'),\n",
        "    tf.keras.layers.RandomZoom(height_factor=(-0.2, -0.1),\n",
        "                               input_shape=INPUT_SHAPE,\n",
        "                               name='Random_Zoom'),\n",
        "\n",
        "\n",
        "    # 1st convolutional block\n",
        "    Conv2D(filters=96,\n",
        "           kernel_size=(7,7),\n",
        "           strides=(2,2),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_1'),\n",
        "    ReLU(name='ReLU_1'),\n",
        "    MaxPooling2D(pool_size=(3,3),\n",
        "                 strides=(2,2),\n",
        "                 padding='valid',\n",
        "                 name='Max_Pooling_1'),\n",
        "\n",
        "    # 2nd convolutional block\n",
        "    Conv2D(filters=256,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(2,2),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_2'),\n",
        "    ReLU(name='ReLU_2'),\n",
        "    MaxPooling2D(pool_size=(3,3),\n",
        "                 strides=(2,2),\n",
        "                 padding='valid',\n",
        "                 name='Max_Pooling_2'),\n",
        "\n",
        "    # 3rd convolutional block\n",
        "    Conv2D(filters=384,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(1,1),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_3.1'),\n",
        "    ReLU(name='ReLU_3.1'),\n",
        "    Conv2D(filters=384,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(1,1),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_3.2'),\n",
        "    ReLU(name='ReLU_3.2'),\n",
        "    Conv2D(filters=256,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(1,1),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_3.3'),\n",
        "    ReLU(name='ReLU_3.3'),\n",
        "    MaxPooling2D(pool_size=(3,3),\n",
        "                 strides=(2,2),\n",
        "                 padding='valid',\n",
        "                 name='Max_Pooling_3'),\n",
        "\n",
        "    # Flatten Layer\n",
        "    Flatten(name='Flatten'),\n",
        "\n",
        "    # 1st Dense Layer\n",
        "    Dense(units=1024,\n",
        "          activation='relu',\n",
        "          name='Dense_1'),\n",
        "\n",
        "    # 2nd Dense Layer\n",
        "    Dense(units=1024,\n",
        "          activation='relu',\n",
        "          name='Dense_2'),\n",
        "\n",
        "    # 3rd Dense Layer\n",
        "    Dense(units=num_classes,\n",
        "          activation='softmax',\n",
        "          name=\"Output\")\n",
        "],\n",
        "    name='ZFNet')"
      ],
      "metadata": {
        "id": "dPhZ7yTvo9TT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet.build(input_shape=(None, *INPUT_SHAPE))\n",
        "zfnet.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87NX1WgU35jU",
        "outputId": "6d2b215e-bff9-4282-9cc8-f8a5e0770fa5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ZFNet\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling_9 (Rescaling)     (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " Random_Horizontal_And_Vert  (None, 224, 224, 3)       0         \n",
            " ical_Flip (RandomFlip)                                          \n",
            "                                                                 \n",
            " Random_Rotation (RandomRot  (None, 224, 224, 3)       0         \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " Random_Contrast (RandomCon  (None, 224, 224, 3)       0         \n",
            " trast)                                                          \n",
            "                                                                 \n",
            " Random_Zoom (RandomZoom)    (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " Conv_1 (Conv2D)             (None, 109, 109, 96)      14208     \n",
            "                                                                 \n",
            " ReLU_1 (ReLU)               (None, 109, 109, 96)      0         \n",
            "                                                                 \n",
            " Max_Pooling_1 (MaxPooling2  (None, 54, 54, 96)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " Conv_2 (Conv2D)             (None, 26, 26, 256)       221440    \n",
            "                                                                 \n",
            " ReLU_2 (ReLU)               (None, 26, 26, 256)       0         \n",
            "                                                                 \n",
            " Max_Pooling_2 (MaxPooling2  (None, 12, 12, 256)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " Conv_3.1 (Conv2D)           (None, 10, 10, 384)       885120    \n",
            "                                                                 \n",
            " ReLU_3.1 (ReLU)             (None, 10, 10, 384)       0         \n",
            "                                                                 \n",
            " Conv_3.2 (Conv2D)           (None, 8, 8, 384)         1327488   \n",
            "                                                                 \n",
            " ReLU_3.2 (ReLU)             (None, 8, 8, 384)         0         \n",
            "                                                                 \n",
            " Conv_3.3 (Conv2D)           (None, 6, 6, 256)         884992    \n",
            "                                                                 \n",
            " ReLU_3.3 (ReLU)             (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " Max_Pooling_3 (MaxPooling2  (None, 2, 2, 256)         0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " Flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " Dense_1 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " Dense_2 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " Output (Dense)              (None, 4)                 4100      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5436548 (20.74 MB)\n",
            "Trainable params: 5436548 (20.74 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model compiling."
      ],
      "metadata": {
        "id": "54ThNT8aK3Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet.compile(loss=SparseCategoricalCrossentropy(),\n",
        "              optimizer=Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "jNQQIG-yJ_Dc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create csv logger to save the results in case it stops."
      ],
      "metadata": {
        "id": "_rE5T_4bK5Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet_csv_logger = CSVLogger(\"/content/model_logs/zfnet.log\")"
      ],
      "metadata": {
        "id": "Z4iXDMXhKZ6E"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "nvReb7aZK9Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet_history = zfnet.fit(train_ds,\n",
        "                          epochs=30,\n",
        "                          steps_per_epoch=len(train_ds),\n",
        "                          validation_data=test_ds,\n",
        "                          validation_steps=len(test_ds),\n",
        "                          class_weight=class_weights,\n",
        "                          callbacks=zfnet_csv_logger)"
      ],
      "metadata": {
        "id": "Ft1tUhCWK_j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Model 2:"
      ],
      "metadata": {
        "id": "GGyQtP-3MAB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the folders"
      ],
      "metadata": {
        "id": "qBn6tAjx39E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define source and destination paths\n",
        "source_path = '/content/data'\n",
        "destination_path = '/content/drive/My Drive/AMD/data'\n",
        "\n",
        "# Copy the folder to Google Drive\n",
        "shutil.copytree(source_path, destination_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "57W_ZcG2g6n6",
        "outputId": "198f9b5b-5d33-49f5-dcdb-fd373e362a14"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/AMD/data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}