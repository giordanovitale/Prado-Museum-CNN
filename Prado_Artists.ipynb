{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "L8kbFRoYj3BI",
        "qBn6tAjx39E7"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSRQMSVlTLMSk3mNoAS1EK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giordanovitale/Prado-Museum-CNN/blob/main/Prado_Artists.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   [Data Augmentation Techniques](https://medium.com/ymedialabs-innovation/data-augmentation-techniques-in-cnn-using-tensorflow-371ae43d5be9#8be0)\n",
        "2.   [Model Architectures](https://medium.com/@navarai/unveiling-the-diversity-a-comprehensive-guide-to-types-of-cnn-architectures-9d70da0b4521)\n",
        "3. [EfficientNet](https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142)"
      ],
      "metadata": {
        "id": "WbjcyTneoQAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 - Load the necessary libraries"
      ],
      "metadata": {
        "id": "o8R_We7Bjw9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Source: https://www.kaggle.com/datasets/maparla/prado-museum-pictures"
      ],
      "metadata": {
        "id": "wzU4x8i_tk_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7FrmpviCjilL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import polars as pl\n",
        "\n",
        "import tqdm\n",
        "import os\n",
        "import requests\n",
        "\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "# import visualkeras as vk\n",
        "\n",
        "from scipy.optimize import fsolve\n",
        "from math import exp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.layers import Layer\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, \\\n",
        "    AveragePooling2D, BatchNormalization, ReLU, PReLU, ZeroPadding2D, \\\n",
        "    GlobalAveragePooling2D, Input, DepthwiseConv2D, Add, Activation, Lambda, RandomFlip, \\\n",
        "    Rescaling, RandomContrast, RandomZoom\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_v2_preproccessing\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as efficientnet_preproccessing\n",
        "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B3\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg_preproccessing\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocessing\n",
        "\n",
        "# Get rid of the extra stuff that gets downloaded\n",
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (224,224,3)"
      ],
      "metadata": {
        "id": "O4u7vtjkI7bk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Helper Functions"
      ],
      "metadata": {
        "id": "L8kbFRoYj3BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_url(args):\n",
        "    \"\"\"\n",
        "    Downloads a file from an url\n",
        "    :param args: Tuple containing the url and filename\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    url, filename = args[0], args[1]\n",
        "    try:\n",
        "      r = requests.get(url)\n",
        "      if r.status_code != 404:\n",
        "        with open(filename, \"wb\") as f:\n",
        "          f.write(r.content)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"Exception in download_url():\", e)"
      ],
      "metadata": {
        "id": "OYyBBIO-jvgM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_parallel(args):\n",
        "    \"\"\"\n",
        "    Downloads urls in parallel\n",
        "    :param args: List of tuples containing the url and filename\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    cpus = cpu_count()\n",
        "    threadpool = ThreadPool(cpus)\n",
        "    with tqdm.tqdm(total=len(args), desc=\"Downloading images\") as pbar:\n",
        "        for _ in threadpool.imap_unordered(download_url, args):\n",
        "            pbar.update(1)\n",
        "    threadpool.close()\n",
        "    threadpool.join()"
      ],
      "metadata": {
        "id": "6lOeid62TQSE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Load the dataset using Kaggle API"
      ],
      "metadata": {
        "id": "MaMrNaJvoe2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Username and Key have been secreted. Replace `userdata.get('KAGGLE_USERNAME')` and `userdata.get('KAGGLE_KEY')`with your username and key, respectively."
      ],
      "metadata": {
        "id": "wZrGs5uD2s0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"KAGGLE_USERNAME\"] = \"userdata.get('KAGGLE_USERNAME')\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"userdata.get('KAGGLE_KEY')\"\n",
        "!kaggle datasets download maparla/prado-museum-pictures -f prado.csv\n",
        "!unzip prado.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBj68-vEvZLa",
        "outputId": "8ae98597-4ed0-40fd-9797-da92115d1910"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/maparla/prado-museum-pictures\n",
            "License(s): MIT\n",
            "Downloading prado.csv.zip to /content\n",
            " 49% 9.00M/18.3M [00:00<00:00, 12.8MB/s]\n",
            "100% 18.3M/18.3M [00:00<00:00, 25.0MB/s]\n",
            "Archive:  prado.csv.zip\n",
            "  inflating: prado.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataframe from the unzipepd csv file."
      ],
      "metadata": {
        "id": "RZ4NWN4a26ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(\"prado.csv\"))"
      ],
      "metadata": {
        "id": "FRAesYqqwHCr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since no target class has been defined from the project assignment, I have to decide it. After a careful inspection of the columns, I found out that the more suitable ones are `author` and `technical_sheet_tecnica`. The latter seems more intriguing as it has more observations, thus being more suitable to big data algorithms."
      ],
      "metadata": {
        "id": "d60cuROa2-o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['author'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKYs7m-XwOyB",
        "outputId": "f441ad4e-9d91-4c6f-f5dd-94acd095c2d1"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author\n",
              "Anónimo                                                                       2698\n",
              "Goya y Lucientes, Francisco de                                                1080\n",
              "Bayeu y Subías, Francisco                                                      446\n",
              "Haes, Carlos de                                                                326\n",
              "Pizarro y Librado, Cecilio                                                     290\n",
              "                                                                              ... \n",
              "Malombra, Pietro                                                                 1\n",
              "Taller de Bellini, Giovanni                                                      1\n",
              "Mattioli, Ludovico -Dibujante- (Autor de la obra original: Cignani, Carlo)       1\n",
              "Ricci, Marco                                                                     1\n",
              "García, Sergio                                                                   1\n",
              "Name: count, Length: 2560, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['technical_sheet_tecnica'].value_counts().sort_values(ascending=False)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD4-MW-bwPga",
        "outputId": "15f34df9-309d-4242-f7ef-e76c6c0bda4b"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception in download_url(): [Errno 2] No such file or directory: 'data/pencil/59825391-c4b9-4f84-8d6f-413fa045a3ea.jpg'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "technical_sheet_tecnica\n",
              "Óleo                    4156\n",
              "Acuñación               1118\n",
              "Esculpido                550\n",
              "Lápiz compuesto          476\n",
              "Clarión; Lápiz negro     396\n",
              "Albúmina                 395\n",
              "Sanguina                 372\n",
              "Lápiz                    259\n",
              "Lápiz negro              237\n",
              "Pluma; Tinta parda       214\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce the data set by keeping observations belonging to the 4 classes of interest only."
      ],
      "metadata": {
        "id": "RaNVQhcOcjAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['technical_sheet_tecnica'].isin(['Óleo',\n",
        "                                            'Acuñación',\n",
        "                                            'Esculpido',\n",
        "                                            'Lápiz compuesto'])]"
      ],
      "metadata": {
        "id": "JjRARJodH5or"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B467V520Iwcn",
        "outputId": "ade3207b-4d2c-4c2d-86fd-c08dc99ef0b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6300, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to obtain the JPGs images, we need to start from the given URL column `work_image_url`."
      ],
      "metadata": {
        "id": "0iHqoZHs4btw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['work_id'] = df['work_image_url'].apply(lambda x: x.split('/')[-1])"
      ],
      "metadata": {
        "id": "bvORo-571oOX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the folders into which the images will be stored, according to their respective class."
      ],
      "metadata": {
        "id": "GO_mspTpBiTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The important columns are:\n",
        "`['work_image_url', 'work_id', 'technical_sheet_tecnica']`"
      ],
      "metadata": {
        "id": "FC_ZTuARczzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "NvXIGlFrRsRR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "esp_techniques = ['Óleo', 'Acuñación', 'Esculpido', 'Lápiz compuesto']\n",
        "eng_techniques = ['oil', 'minting', 'sculpture', 'pencil']"
      ],
      "metadata": {
        "id": "0MzHBZFvTgpP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rm -rf data"
      ],
      "metadata": {
        "id": "95J0G6lWUtRU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for technique in eng_techniques:\n",
        "  !mkdir -p data/$technique"
      ],
      "metadata": {
        "id": "dUdTNYO1RzSa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model_logs"
      ],
      "metadata": {
        "id": "1i5NdKk6dNVq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJAjz_Hfczwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BF5ZPrAEeEHL"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05d-eS5ynl5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_list = list()\n",
        "\n",
        "for esp_technique, eng_technique in zip(esp_techniques, eng_techniques):\n",
        "\n",
        "  image_urls = list(df['work_image_url'].loc[df['technical_sheet_tecnica'] == esp_technique])\n",
        "  image_names = list(df['work_id'].loc[df['technical_sheet_tecnica'] == esp_technique])\n",
        "  image_names = [f\"data/{eng_technique}/\" + fn for fn in image_names]\n",
        "\n",
        "  for url, fn in zip(image_urls, image_names):\n",
        "    download_list.append((url, fn))"
      ],
      "metadata": {
        "id": "LgleWlJtoExf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mI9BfgmV9S9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(download_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbWz_pknorCZ",
        "outputId": "e762ca34-9391-46a0-fa12-d6aba06b3008"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6300"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_parallel(download_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFc2YbyFosxg",
        "outputId": "2482bb73-b580-431d-c268-9472a00254e8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading images: 100%|██████████| 6300/6300 [2:03:55<00:00,  1.18s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Imbalance"
      ],
      "metadata": {
        "id": "Z_MTwn-YQ37H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights"
      ],
      "metadata": {
        "id": "McYkNoNiMZ40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_oils = df.loc[df['technical_sheet_tecnica']=='Óleo'].shape[0]\n",
        "num_mintings = df.loc[df['technical_sheet_tecnica']=='Acuñación'].shape[0]\n",
        "num_sculptures = df.loc[df['technical_sheet_tecnica']=='Esculpido'].shape[0]\n",
        "num_pencils = df.loc[df['technical_sheet_tecnica']=='Lápiz compuesto'].shape[0]\n",
        "\n",
        "num_total = df.shape[0]"
      ],
      "metadata": {
        "id": "Ds6y2fbTMz9w"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = {\n",
        "    0: num_oils/num_total,\n",
        "    1: num_mintings/num_total,\n",
        "    2: num_sculptures/num_total,\n",
        "    3: num_pencils/num_total\n",
        "}\n",
        "\n",
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDE6xgiyMCSn",
        "outputId": "799d9763-8204-43c7-d2b1-cb043b858873"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.6596825396825396,\n",
              " 1: 0.17746031746031746,\n",
              " 2: 0.0873015873015873,\n",
              " 3: 0.07555555555555556}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Temporarily use loaded data and mount"
      ],
      "metadata": {
        "id": "_XcEZWRU7j8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/AMD\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drTVncya7sQf",
        "outputId": "da6b22b5-ebb6-4d20-9df0-b9d1fb4c8fcb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "data  model_logs  Prado-Artists.ipynb  prado.csv  prado.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = tf.keras.utils.image_dataset_from_directory('/content/drive/MyDrive/AMD/data',\n",
        "                                                                label_mode='int',\n",
        "                                                                color_mode='rgb',\n",
        "                                                                batch_size=32,\n",
        "                                                                image_size=(224, 224),\n",
        "                                                                shuffle=True,\n",
        "                                                                seed=42,\n",
        "                                                                validation_split=0.2,\n",
        "                                                                subset='both',\n",
        "                                                                labels='inferred',\n",
        "                                                                class_names=['oil', 'minting', 'sculpture', 'pencil'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUmID9Kk80Yv",
        "outputId": "7f77c5cb-d82b-4c09-822b-f3a079d662a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5935 files belonging to 4 classes.\n",
            "Using 4748 files for training.\n",
            "Using 1187 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Model 1: ZFNet"
      ],
      "metadata": {
        "id": "jyowUsuDJFrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/images/data_augmentation"
      ],
      "metadata": {
        "id": "dhN1PODLEN6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the link above:\n",
        "\n",
        "> Note: Data augmentation is inactive at test time so input images will only be augmented during calls to Model.fit (not Model.evaluate or Model.predict).\n",
        "\n"
      ],
      "metadata": {
        "id": "eoZ2PpzPGKbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4"
      ],
      "metadata": {
        "id": "-lewV5opGf1U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet = Sequential([\n",
        "\n",
        "    # Preprocessing layers\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.RandomFlip(mode='horizontal_and_vertical',\n",
        "                               input_shape=INPUT_SHAPE,\n",
        "                               name='Random_Horizontal_And_Vertical_Flip'),\n",
        "    tf.keras.layers.RandomRotation(factor=0.2,\n",
        "                                   input_shape=INPUT_SHAPE,\n",
        "                                   name='Random_Rotation'),\n",
        "    tf.keras.layers.RandomContrast(factor=0.2,\n",
        "                                   input_shape=INPUT_SHAPE,\n",
        "                                   name='Random_Contrast'),\n",
        "    tf.keras.layers.RandomZoom(height_factor=(-0.2, -0.1),\n",
        "                               input_shape=INPUT_SHAPE,\n",
        "                               name='Random_Zoom'),\n",
        "\n",
        "\n",
        "    # 1st convolutional block\n",
        "    Conv2D(filters=48,\n",
        "           kernel_size=(7,7),\n",
        "           strides=(2,2),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_1'),\n",
        "    ReLU(name='ReLU_1'),\n",
        "    MaxPooling2D(pool_size=(3,3),\n",
        "                 strides=(2,2),\n",
        "                 padding='valid',\n",
        "                 name='Max_Pooling_1'),\n",
        "\n",
        "    # 2nd convolutional block\n",
        "    Conv2D(filters=128,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(2,2),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_2'),\n",
        "    ReLU(name='ReLU_2'),\n",
        "    MaxPooling2D(pool_size=(3,3),\n",
        "                 strides=(2,2),\n",
        "                 padding='valid',\n",
        "                 name='Max_Pooling_2'),\n",
        "\n",
        "    # 3rd convolutional block\n",
        "    Conv2D(filters=192,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(1,1),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_3.1'),\n",
        "    ReLU(name='ReLU_3.1'),\n",
        "    Conv2D(filters=384,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(1,1),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_3.2'),\n",
        "    ReLU(name='ReLU_3.2'),\n",
        "    Conv2D(filters=192,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(1,1),\n",
        "           padding='valid',\n",
        "           activation=None,\n",
        "           name='Conv_3.3'),\n",
        "    ReLU(name='ReLU_3.3'),\n",
        "    MaxPooling2D(pool_size=(3,3),\n",
        "                 strides=(2,2),\n",
        "                 padding='valid',\n",
        "                 name='Max_Pooling_3'),\n",
        "\n",
        "    # Flatten Layer\n",
        "    Flatten(name='Flatten'),\n",
        "\n",
        "    # 1st Dense Layer\n",
        "    Dense(units=256,\n",
        "          activation='relu',\n",
        "          name='Dense_1'),\n",
        "\n",
        "    # 2nd Dense Layer\n",
        "    Dense(units=256,\n",
        "          activation='relu',\n",
        "          name='Dense_2'),\n",
        "\n",
        "    # 3rd Dense Layer\n",
        "    Dense(units=num_classes,\n",
        "          activation='softmax',\n",
        "          name=\"Output\")\n",
        "],\n",
        "    name='ZFNet')"
      ],
      "metadata": {
        "id": "dPhZ7yTvo9TT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet.build(input_shape=(None, *INPUT_SHAPE))\n",
        "zfnet.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87NX1WgU35jU",
        "outputId": "0838766f-6938-4a88-ca61-98027fd77bd4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ZFNet\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling_3 (Rescaling)     (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " Random_Horizontal_And_Vert  (None, 224, 224, 3)       0         \n",
            " ical_Flip (RandomFlip)                                          \n",
            "                                                                 \n",
            " Random_Rotation (RandomRot  (None, 224, 224, 3)       0         \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " Random_Contrast (RandomCon  (None, 224, 224, 3)       0         \n",
            " trast)                                                          \n",
            "                                                                 \n",
            " Random_Zoom (RandomZoom)    (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " Conv_1 (Conv2D)             (None, 109, 109, 48)      7104      \n",
            "                                                                 \n",
            " ReLU_1 (ReLU)               (None, 109, 109, 48)      0         \n",
            "                                                                 \n",
            " Max_Pooling_1 (MaxPooling2  (None, 54, 54, 48)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " Conv_2 (Conv2D)             (None, 26, 26, 128)       55424     \n",
            "                                                                 \n",
            " ReLU_2 (ReLU)               (None, 26, 26, 128)       0         \n",
            "                                                                 \n",
            " Max_Pooling_2 (MaxPooling2  (None, 12, 12, 128)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " Conv_3.1 (Conv2D)           (None, 10, 10, 192)       221376    \n",
            "                                                                 \n",
            " ReLU_3.1 (ReLU)             (None, 10, 10, 192)       0         \n",
            "                                                                 \n",
            " Conv_3.2 (Conv2D)           (None, 8, 8, 384)         663936    \n",
            "                                                                 \n",
            " ReLU_3.2 (ReLU)             (None, 8, 8, 384)         0         \n",
            "                                                                 \n",
            " Conv_3.3 (Conv2D)           (None, 6, 6, 192)         663744    \n",
            "                                                                 \n",
            " ReLU_3.3 (ReLU)             (None, 6, 6, 192)         0         \n",
            "                                                                 \n",
            " Max_Pooling_3 (MaxPooling2  (None, 2, 2, 192)         0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " Flatten (Flatten)           (None, 768)               0         \n",
            "                                                                 \n",
            " Dense_1 (Dense)             (None, 256)               196864    \n",
            "                                                                 \n",
            " Dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " Output (Dense)              (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1875268 (7.15 MB)\n",
            "Trainable params: 1875268 (7.15 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model compiling."
      ],
      "metadata": {
        "id": "54ThNT8aK3Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet.compile(loss=SparseCategoricalCrossentropy(),\n",
        "              optimizer=Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "jNQQIG-yJ_Dc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create csv logger to save the results in case it stops."
      ],
      "metadata": {
        "id": "_rE5T_4bK5Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet_csv_logger = CSVLogger(\"/content/model_logs/zfnet.log\")"
      ],
      "metadata": {
        "id": "Z4iXDMXhKZ6E"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "nvReb7aZK9Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zfnet_history = zfnet.fit(train_ds,\n",
        "                          epochs=30,\n",
        "                          steps_per_epoch=len(train_ds),\n",
        "                          validation_data=test_ds,\n",
        "                          validation_steps=len(test_ds),\n",
        "                          class_weight=class_weights,\n",
        "                          callbacks=zfnet_csv_logger)"
      ],
      "metadata": {
        "id": "Ft1tUhCWK_j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Model 2: My Own Proposal"
      ],
      "metadata": {
        "id": "GGyQtP-3MAB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gionet = Sequential([\n",
        "\n",
        "    # Preprocessing layers\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.RandomFlip(mode='horizontal_and_vertical',\n",
        "                               input_shape=INPUT_SHAPE,\n",
        "                               name='Random_Horizontal_And_Vertical_Flip'),\n",
        "    tf.keras.layers.RandomRotation(factor=0.2,\n",
        "                                   input_shape=INPUT_SHAPE,\n",
        "                                   name='Random_Rotation'),\n",
        "    tf.keras.layers.RandomContrast(factor=0.2,\n",
        "                                   input_shape=INPUT_SHAPE,\n",
        "                                   name='Random_Contrast'),\n",
        "    tf.keras.layers.RandomZoom(height_factor=(-0.2, -0.1),\n",
        "                               input_shape=INPUT_SHAPE,\n",
        "                               name='Random_Zoom'),\n",
        "    tf.keras.layers.Lambda(function=tf.image.per_image_standardization,\n",
        "                           name=\"Per_image_standardisation\"),\n",
        "\n",
        "\n",
        "    # 1st convolutional block\n",
        "    Conv2D(filters=112,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(2,2),\n",
        "           padding='same',\n",
        "           activation=None,\n",
        "           name='Conv_1'),\n",
        "    ReLU(name='ReLU_1'),\n",
        "    AveragePooling2D(pool_size=(3,3),\n",
        "                     strides=(2,2),\n",
        "                     padding='same',\n",
        "                     name='Avg_Pooling_1'),\n",
        "\n",
        "    # 2nd convolutional block\n",
        "    Conv2D(filters=56,\n",
        "           kernel_size=(3,3),\n",
        "           strides=(2,2),\n",
        "           padding='same',\n",
        "           activation=None,\n",
        "           name='Conv_2'),\n",
        "    ReLU(name='ReLU_2'),\n",
        "    AveragePooling2D(pool_size=(3,3),\n",
        "                     strides=(2,2),\n",
        "                     padding='same',\n",
        "                     name='Avg_Pooling_2'),\n",
        "\n",
        "    # 3rd convolutional block\n",
        "    Conv2D(filters=28,\n",
        "           kernel_size=(5,5),\n",
        "           strides=(1,1),\n",
        "           padding='same',\n",
        "           activation=None,\n",
        "           name='Conv_3.1'),\n",
        "    ReLU(name='ReLU_3.1'),\n",
        "    Conv2D(filters=28,\n",
        "           kernel_size=(5,5),\n",
        "           strides=(1,1),\n",
        "           padding='same',\n",
        "           activation=None,\n",
        "           name='Conv_3.2'),\n",
        "    ReLU(name='ReLU_3.2'),\n",
        "    AveragePooling2D(pool_size=(3,3),\n",
        "                     strides=(2,2),\n",
        "                     padding='valid',\n",
        "                     name='Avg_Pooling_3'),\n",
        "\n",
        "    # Flatten Layer\n",
        "    Flatten(name='Flatten'),\n",
        "\n",
        "    # 1st Dense Layer\n",
        "    Dense(units=64,\n",
        "          activation='relu',\n",
        "          name='Dense_1'),\n",
        "\n",
        "    # 2nd Dense Layer\n",
        "    Dense(units=num_classes,\n",
        "          activation='softmax',\n",
        "          name=\"Output\")\n",
        "],\n",
        "    name='GioNet')"
      ],
      "metadata": {
        "id": "yH6WDioMqWCk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gionet.build(input_shape=(None, *INPUT_SHAPE))\n",
        "gionet.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYPt3WdIv2B2",
        "outputId": "0efa3f8a-f14b-4f20-e824-8af2e81b1c00"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"GioNet\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling_4 (Rescaling)     (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " Random_Horizontal_And_Vert  (None, 224, 224, 3)       0         \n",
            " ical_Flip (RandomFlip)                                          \n",
            "                                                                 \n",
            " Random_Rotation (RandomRot  (None, 224, 224, 3)       0         \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " Random_Contrast (RandomCon  (None, 224, 224, 3)       0         \n",
            " trast)                                                          \n",
            "                                                                 \n",
            " Random_Zoom (RandomZoom)    (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " Per_image_standardisation   (None, 224, 224, 3)       0         \n",
            " (Lambda)                                                        \n",
            "                                                                 \n",
            " Conv_1 (Conv2D)             (None, 112, 112, 112)     3136      \n",
            "                                                                 \n",
            " ReLU_1 (ReLU)               (None, 112, 112, 112)     0         \n",
            "                                                                 \n",
            " Avg_Pooling_1 (AveragePool  (None, 56, 56, 112)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " Conv_2 (Conv2D)             (None, 28, 28, 56)        56504     \n",
            "                                                                 \n",
            " ReLU_2 (ReLU)               (None, 28, 28, 56)        0         \n",
            "                                                                 \n",
            " Avg_Pooling_2 (AveragePool  (None, 14, 14, 56)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " Conv_3.1 (Conv2D)           (None, 14, 14, 28)        39228     \n",
            "                                                                 \n",
            " ReLU_3.1 (ReLU)             (None, 14, 14, 28)        0         \n",
            "                                                                 \n",
            " Conv_3.2 (Conv2D)           (None, 14, 14, 28)        19628     \n",
            "                                                                 \n",
            " ReLU_3.2 (ReLU)             (None, 14, 14, 28)        0         \n",
            "                                                                 \n",
            " Avg_Pooling_3 (AveragePool  (None, 6, 6, 28)          0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " Flatten (Flatten)           (None, 1008)              0         \n",
            "                                                                 \n",
            " Dense_1 (Dense)             (None, 64)                64576     \n",
            "                                                                 \n",
            " Output (Dense)              (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 183332 (716.14 KB)\n",
            "Trainable params: 183332 (716.14 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model"
      ],
      "metadata": {
        "id": "l4KZU8Bfv7u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gionet.compile(loss=SparseCategoricalCrossentropy(),\n",
        "               optimizer=Adam(),\n",
        "               metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "si8p-R4Sv9U9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV logger in case it stops."
      ],
      "metadata": {
        "id": "Uwf3LuWowV-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gionet_csv_logger = CSVLogger(\"/content/model_results/gionet.log\")"
      ],
      "metadata": {
        "id": "x9ZpkVvdwXuF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "dggLpZbpxFpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gionet_history = gionet.fit(train_ds,\n",
        "                            epochs=30,\n",
        "                            steps_per_epoch=len(train_ds),\n",
        "                            validation_data=test_ds,\n",
        "                            validation_steps=len(test_ds),\n",
        "                            class_weight=class_weights,\n",
        "                            callbacks=gionet_csv_logger)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "U1Lpc0ylxHDW",
        "outputId": "761f6b80-b361-4625-fa9b-a483a52b1a2d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "149/149 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.8608"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "/content/model_results/gionet.log; No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-fe9256b86e75>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m gionet_history = gionet.fit(train_ds,\n\u001b[0m\u001b[1;32m      2\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/csv.py\u001b[0m in \u001b[0;36mwriteheader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwriteheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dict_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/csv.py\u001b[0m in \u001b[0;36mwriterow\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: /content/model_results/gionet.log; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the folders"
      ],
      "metadata": {
        "id": "qBn6tAjx39E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define source and destination paths\n",
        "source_path = '/content/data'\n",
        "destination_path = '/content/drive/My Drive/AMD/data'\n",
        "\n",
        "# Copy the folder to Google Drive\n",
        "shutil.copytree(source_path, destination_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "57W_ZcG2g6n6",
        "outputId": "198f9b5b-5d33-49f5-dcdb-fd373e362a14"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/AMD/data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}